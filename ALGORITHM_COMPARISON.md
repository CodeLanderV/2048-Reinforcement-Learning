# Algorithm Comparison: 2048 Reinforcement Learning

## Overview

This project implements **4 different algorithms** for playing 2048. Each has unique characteristics and trade-offs.

---

## üß† Algorithm Comparison Table

| Algorithm | Type | Learning | Policy | Memory | Network | Best For |
|-----------|------|----------|--------|--------|---------|----------|
| **DQN** | Value-Based | Off-Policy | Œµ-greedy | Replay Buffer | Q-Network | Stable, sample-efficient learning |
| **Double DQN** | Value-Based | Off-Policy | Œµ-greedy | Replay Buffer | 2 Q-Networks | Reducing Q-value overestimation |
| **MCTS** | Planning | None | UCB | None | None | No training data, strong baseline |
| **REINFORCE** | Policy Gradient | On-Policy | Stochastic œÄ(a\|s) | Episode Buffer | Policy Network | Direct policy optimization |

---

## üìä Detailed Algorithm Descriptions

### 1. DQN (Deep Q-Network)

**Type:** Value-based reinforcement learning  
**Learning:** Off-policy (learns from any experience)  
**Policy:** Œµ-greedy (explores with probability Œµ)

#### Key Features:
- **Q-Value Function:** Learns `Q(s,a)` = expected return for taking action `a` in state `s`
- **Bellman Update:** `Q(s,a) = r + Œ≥ * max Q_target(s', a')`
- **Experience Replay:** Stores transitions in buffer, samples randomly for training
- **Target Network:** Separate network for stable Q-value targets (updated every 1000 steps)
- **Epsilon Decay:** Exploration rate decreases exponentially over time

#### Advantages:
- ‚úÖ Sample efficient (reuses experience)
- ‚úÖ Stable learning with target networks
- ‚úÖ Off-policy (can learn from demonstrations)

#### Disadvantages:
- ‚ùå Can overestimate Q-values (fixed by Double DQN)
- ‚ùå Requires careful hyperparameter tuning

#### When to Use:
- Standard baseline for value-based RL
- When you have limited data
- When you need stable, predictable training

---

### 2. Double DQN

**Type:** Value-based reinforcement learning (improved DQN)  
**Learning:** Off-policy  
**Policy:** Œµ-greedy

#### Key Features:
- **Decoupled Selection/Evaluation:** 
  - Policy network **selects** action: `a* = argmax Q_policy(s', a)`
  - Target network **evaluates** action: `Q_target(s', a*)`
- **Reduces Overestimation:** Addresses DQN's tendency to overestimate Q-values
- **Same Architecture as DQN:** Just changes the target calculation

#### Advantages:
- ‚úÖ All DQN advantages
- ‚úÖ More accurate Q-value estimates
- ‚úÖ Often achieves better final performance

#### Disadvantages:
- ‚ùå Slightly more complex than DQN
- ‚ùå Still requires hyperparameter tuning

#### When to Use:
- **Always prefer over vanilla DQN** (strictly better)
- When Q-value accuracy matters
- When you want state-of-the-art value-based performance

---

### 3. MCTS (Monte Carlo Tree Search)

**Type:** Planning algorithm (no learning)  
**Learning:** None  
**Policy:** Implicit through tree search

#### Key Features:
- **UCB Tree Search:** Uses Upper Confidence Bound to balance exploration/exploitation
- **Four Phases:**
  1. **Selection:** Traverse tree using UCB until reaching unexpanded node
  2. **Expansion:** Add new child node for unexplored action
  3. **Simulation:** Random playout from new node until game over
  4. **Backpropagation:** Update visit counts and values up the tree
- **No Neural Network:** Pure algorithmic approach
- **No Memory:** Builds fresh tree for each move

#### Advantages:
- ‚úÖ No training required (works immediately)
- ‚úÖ No hyperparameters to tune (just simulations count)
- ‚úÖ Guaranteed to improve with more compute
- ‚úÖ Strong baseline performance

#### Disadvantages:
- ‚ùå Computationally expensive per move
- ‚ùå Doesn't learn from experience
- ‚ùå Scales poorly to large action spaces

#### When to Use:
- Quick evaluation without training
- Strong baseline to compare against
- When you have computation time per move but no training time
- Domains where perfect simulation is possible

---

### 4. REINFORCE (Monte Carlo Policy Gradient)

**Type:** Policy gradient reinforcement learning  
**Learning:** On-policy (learns only from its own experience)  
**Policy:** Stochastic œÄ(a|s) - outputs probability distribution over actions

#### Key Features:
- **Direct Policy Optimization:** Learns policy œÄ(a|s) directly (not via Q-values)
- **Policy Gradient Theorem:** `‚àáJ(Œ∏) = E[‚àálog œÄ(a|s) * G_t]`
- **Monte Carlo Returns:** Uses full episode returns `G_t = Œ£ Œ≥^k * r_{t+k}`
- **Stochastic Policy:** Naturally explores through probability distribution
- **Episode-Based Updates:** Updates after each complete episode

#### Advantages:
- ‚úÖ Directly optimizes policy (no Q-value intermediary)
- ‚úÖ Natural exploration through stochastic policy
- ‚úÖ Can learn stochastic optimal policies
- ‚úÖ Works well in continuous action spaces

#### Disadvantages:
- ‚ùå High variance in gradient estimates
- ‚ùå Sample inefficient (on-policy only)
- ‚ùå Requires full episodes (can't update mid-game)
- ‚ùå Slower convergence than value-based methods

#### When to Use:
- When you need stochastic policies
- Problems where optimal policy is stochastic
- As foundation for advanced policy gradient methods (A3C, PPO, TRPO)
- When you want to understand policy gradient theory

---

## üî¨ Mathematical Formulations

### DQN Update Rule
```
Q(s,a) ‚Üê Q(s,a) + Œ± * [r + Œ≥ * max Q_target(s', a') - Q(s,a)]
```

### Double DQN Update Rule
```
a* = argmax_a Q_policy(s', a)
Q(s,a) ‚Üê Q(s,a) + Œ± * [r + Œ≥ * Q_target(s', a*) - Q(s,a)]
```

### MCTS UCB Formula
```
UCB(node) = Q(node) + c * sqrt(log(N_parent) / N_node)
```
Where:
- `Q(node)` = average value (exploitation)
- `c * sqrt(...)` = exploration bonus
- `c = ‚àö2` theoretically optimal

### REINFORCE Policy Gradient
```
‚àáJ(Œ∏) = E_œÑ [Œ£_t ‚àálog œÄ_Œ∏(a_t|s_t) * G_t]
```
Where:
- `G_t = Œ£_{k=0}^{T-t} Œ≥^k * r_{t+k}` (discounted return)
- `‚àálog œÄ_Œ∏(a_t|s_t)` = score function
- Increases probability of actions with high returns

---

## üéØ Performance Characteristics (2048 Game)

### Expected Performance (After Training)

| Algorithm | Max Tile (Avg) | Score (Avg) | Training Time | Inference Speed |
|-----------|----------------|-------------|---------------|-----------------|
| **DQN** | 1024-2048 | 15,000-25,000 | ~2 hours | Fast |
| **Double DQN** | 2048-4096 | 20,000-35,000 | ~2 hours | Fast |
| **MCTS** | 512-1024 | 8,000-15,000 | None | Slow (tree search) |
| **REINFORCE** | 512-1024 | 10,000-18,000 | ~3-4 hours | Fast |

### Training Characteristics

| Algorithm | Sample Efficiency | Stability | Hyperparameter Sensitivity |
|-----------|-------------------|-----------|----------------------------|
| **DQN** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê (Moderate) |
| **Double DQN** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê (Moderate) |
| **MCTS** | N/A | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Minimal) |
| **REINFORCE** | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê (High) |

---

## üöÄ Usage Guide

### Training Each Algorithm

```bash
# DQN
python 2048RL.py train --algorithm dqn --episodes 2000

# Double DQN
python 2048RL.py train --algorithm double-dqn --episodes 2000

# MCTS (no training, just evaluation)
python 2048RL.py train --algorithm mcts --episodes 50

# REINFORCE
python 2048RL.py train --algorithm reinforce --episodes 3000
```

### Playing with Trained Models

```bash
# DQN
python 2048RL.py play --model models/DQN/dqn_final.pth

# Double DQN
python 2048RL.py play --model models/DoubleDQN/double_dqn_final.pth

# REINFORCE
python 2048RL.py play --model models/REINFORCE/reinforce_final.pth

# MCTS (no model needed)
python 2048RL.py train --algorithm mcts --episodes 5
```

---

## üîß Hyperparameter Tuning

### Supported Algorithms
- ‚úÖ DQN
- ‚úÖ Double DQN
- ‚ùå MCTS (no hyperparameters to tune)
- ‚ö†Ô∏è REINFORCE (coming soon)

### Tuning Methods

```bash
# Grid Search (exhaustive)
python hyperparam_tuning.py --algorithm dqn --method grid --trials 20

# Random Search (faster)
python hyperparam_tuning.py --algorithm double-dqn --method random --trials 30

# Optuna (Bayesian optimization - most efficient)
python hyperparam_tuning.py --algorithm dqn --method optuna --trials 50
```

---

## üìö References

### DQN
- Mnih et al. (2015). "Human-level control through deep reinforcement learning." Nature.

### Double DQN
- van Hasselt et al. (2016). "Deep Reinforcement Learning with Double Q-learning." AAAI.

### MCTS
- Browne et al. (2012). "A Survey of Monte Carlo Tree Search Methods." IEEE TCIAIG.
- Kocsis & Szepesv√°ri (2006). "Bandit based Monte-Carlo Planning." ECML.

### REINFORCE
- Williams, R. J. (1992). "Simple statistical gradient-following algorithms for connectionist reinforcement learning." Machine Learning, 8, 229-256.

---

## üéì Learning Path

### For Beginners:
1. **Start with MCTS** - Understand tree search without neural networks
2. **Move to DQN** - Learn value-based RL with neural networks
3. **Try Double DQN** - See how small algorithmic changes improve performance
4. **Explore REINFORCE** - Understand policy gradient methods

### For Researchers:
- Compare all algorithms on same task
- Analyze learning curves and convergence
- Study hyperparameter sensitivity
- Implement advanced variants (A3C, PPO, SAC)

---

## ü§î Which Algorithm Should I Use?

### Quick Decision Tree:

**Need best performance?**  
‚Üí **Double DQN** (most reliable)

**No time for training?**  
‚Üí **MCTS** (works immediately)

**Learning about RL theory?**  
‚Üí **REINFORCE** (foundational algorithm)

**Building on value-based methods?**  
‚Üí **DQN** (standard baseline)

**Need stochastic policy?**  
‚Üí **REINFORCE** (only one with explicit stochastic policy)

---

## üìà Expected Training Curves

### DQN / Double DQN
- **Episodes 0-500:** Rapid improvement (random ‚Üí basic strategy)
- **Episodes 500-1500:** Steady improvement (learning merge patterns)
- **Episodes 1500-2000:** Fine-tuning (approaching optimal play)

### REINFORCE
- **Episodes 0-1000:** High variance, slow improvement
- **Episodes 1000-2500:** Gradual learning of better policies
- **Episodes 2500-3000:** Stabilization (may need more episodes)

### MCTS
- **Immediate performance** (no training curve)
- Performance scales linearly with simulation count

---

*Last Updated: October 15, 2025*
