# ğŸ“Š Logging & Plotting Quick Reference

## ğŸ“ Log Files Generated

### During Training â†’ `evaluations/training.txt`
**What it contains:**
- Every episode's performance metrics
- Real-time convergence status
- Checkpoint save notifications
- New best score events

**Sample output:**
```
Ep 2620 | Reward: 1784.21 | Score:   1386 | MA-100:   1362 (best   1444, Î”= -82.1) | Tile:  128 | Îµ: 0.250 | No-Imp: 694 | Time: 3:44:48
[NEW BEST] Ep  899 | New best score: 4708 | Tile: 512
[CHECKPOINT] Saved: models/DQN/dqn_ep2500.pth
```

### During Testing/Challenge â†’ `evaluations/testing.txt`
**What it contains:**
- Every attempt result
- Score and max tile per game
- Running statistics

**Sample output:**
```
ATTEMPT #5
Attempt #5 Result:
  Score: 1450
  Max Tile: 256
  Average Score: 1380.5
  Best Tile So Far: 512
```

---

## ğŸ“ˆ Plotting Results

### Basic Usage
```bash
# Plot both training and testing results
python plot_results.py

# Plot only training
python plot_results.py --training-only

# Plot only testing
python plot_results.py --testing-only
```

### What You Get

#### Training Plots (4 charts):
1. **Episode vs Max Tile Achieved**
   - Shows highest tile reached in each episode
   - Milestone lines at 128, 256, 512, 1024, 2048

2. **Episode vs Best Score**
   - Tracks new record scores over time
   - Shows when agent achieved breakthroughs

3. **Episode vs Average Score (with MA-100)**
   - Blue line: Average score (last 50 episodes)
   - Red line: MA-100 (convergence metric)
   - Gold star: Best MA-100 achieved

4. **Convergence Metric**
   - Episodes without improvement counter
   - Red line: Convergence threshold (1000)
   - Orange fill: Shows "patience" usage

#### Testing Plots (2 charts):
1. **Attempt vs Max Tile Achieved**
   - Performance across multiple attempts
   - Gold star marks best achievement

2. **Attempt vs Score**
   - Blue: Score per attempt
   - Red dashed: Cumulative average

---

## ğŸ¯ Interpreting the Plots

### Training Plot Insights

**Good Training:**
```
MA-100 steadily increasing â†’ Agent is learning
No-Imp counter stays low â†’ Continuous improvement
Max tiles increasing â†’ Better gameplay strategy
```

**Plateau/Convergence:**
```
MA-100 flat or declining â†’ Agent stopped learning
No-Imp counter approaching 1000 â†’ Will auto-stop soon
Max tiles consistent â†’ Skill ceiling reached
```

**Your Current Situation:**
```
Best MA-100: 1444 (around episode 1930)
Current MA-100: 1362 (episode 2620)
No-Imp counter: 694 â†’ Will stop at 1000
Conclusion: Past peak, should use earlier checkpoint
```

### Testing Plot Insights

**Consistent Agent:**
```
Max tiles mostly similar â†’ Reliable strategy
Scores clustered â†’ Consistent performance
```

**Learning Needed:**
```
Max tiles highly variable â†’ Inconsistent strategy
Low average â†’ Agent struggling
```

---

## ğŸ”§ Workflow

### 1. Start Training
```bash
# New terminal window
python 2048RL.py train --algorithm dqn --episodes 5000 --resume-path models/DQN/dqn_ep2000.pth
```
**Result:** Creates/appends to `evaluations/training.txt`

### 2. Monitor Training (Optional)
```bash
# In another terminal, watch the log
tail -f evaluations/training.txt

# Or check last 50 lines
tail -50 evaluations/training.txt
```

### 3. Run Challenge Mode
```bash
python 2048RL.py play --model models/DQN/dqn_ep2000.pth --challenge 2048 --no-ui
```
**Result:** Creates/appends to `evaluations/testing.txt`

### 4. Generate Plots
```bash
python plot_results.py
```
**Result:** Creates:
- `evaluations/training_plots.png`
- `evaluations/testing_plots.png`

### 5. Analyze Results
- Open the PNG files
- Check convergence status
- Identify best checkpoint
- Decide: continue training, use current model, or adjust hyperparameters

---

## ğŸ“Š Log File Locations

```
2048-Reinforcement-Learning/
â”œâ”€â”€ evaluations/
â”‚   â”œâ”€â”€ training.txt          â† All training episodes
â”‚   â”œâ”€â”€ testing.txt           â† All testing attempts
â”‚   â”œâ”€â”€ training_plots.png    â† Generated by plot_results.py
â”‚   â”œâ”€â”€ testing_plots.png     â† Generated by plot_results.py
â”‚   â””â”€â”€ logs.txt              â† General application logs
â””â”€â”€ plot_results.py           â† Plotting utility
```

---

## ğŸ® Example Session

```bash
# Terminal 1: Start training from best checkpoint
python 2048RL.py train --algorithm dqn --episodes 3000 --resume-path models/DQN/dqn_ep2000.pth

# Terminal 2: Monitor progress
watch -n 10 "tail -20 evaluations/training.txt"

# After training stops or you stop it (Ctrl+C):
python plot_results.py --training-only

# Test the model in challenge mode
python 2048RL.py play --model models/DQN/dqn_final.pth --challenge 2048 --no-ui

# Plot testing results
python plot_results.py --testing-only

# Plot everything
python plot_results.py
```

---

## ğŸ” Key Metrics Explained

| Metric | What It Means | Good Value |
|--------|---------------|------------|
| **MA-100** | Average score over last 100 episodes | Steadily increasing |
| **No-Imp** | Episodes without MA-100 improvement | Low (< 500) |
| **Îµ (Epsilon)** | Exploration rate (randomness) | 0.15-0.25 during training |
| **Max Tile** | Highest tile reached | 512+ is good, 1024+ is excellent |
| **Best Score** | All-time highest score | 4000+ is competitive |

---

## ğŸ’¡ Tips

1. **Training logs are append-only** - Each training session adds to the file
2. **Clear old logs if needed:**
   ```bash
   rm evaluations/training.txt evaluations/testing.txt
   ```
3. **Plot anytime** - Works with partial data, updates as training progresses
4. **Compare runs** - Save plots with different names to compare settings

---

## ğŸš€ Ready to Use!

Everything is set up. Just run your training and the logs will automatically capture all metrics for plotting!
