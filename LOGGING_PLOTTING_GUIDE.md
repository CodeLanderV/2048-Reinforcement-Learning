# 📊 Logging & Plotting Quick Reference

## 📝 Log Files Generated

### During Training → `evaluations/training.txt`
**What it contains:**
- Every episode's performance metrics
- Real-time convergence status
- Checkpoint save notifications
- New best score events

**Sample output:**
```
Ep 2620 | Reward: 1784.21 | Score:   1386 | MA-100:   1362 (best   1444, Δ= -82.1) | Tile:  128 | ε: 0.250 | No-Imp: 694 | Time: 3:44:48
[NEW BEST] Ep  899 | New best score: 4708 | Tile: 512
[CHECKPOINT] Saved: models/DQN/dqn_ep2500.pth
```

### During Testing/Challenge → `evaluations/testing.txt`
**What it contains:**
- Every attempt result
- Score and max tile per game
- Running statistics

**Sample output:**
```
ATTEMPT #5
Attempt #5 Result:
  Score: 1450
  Max Tile: 256
  Average Score: 1380.5
  Best Tile So Far: 512
```

---

## 📈 Plotting Results

### Basic Usage
```bash
# Plot both training and testing results
python plot_results.py

# Plot only training
python plot_results.py --training-only

# Plot only testing
python plot_results.py --testing-only
```

### What You Get

#### Training Plots (4 charts):
1. **Episode vs Max Tile Achieved**
   - Shows highest tile reached in each episode
   - Milestone lines at 128, 256, 512, 1024, 2048

2. **Episode vs Best Score**
   - Tracks new record scores over time
   - Shows when agent achieved breakthroughs

3. **Episode vs Average Score (with MA-100)**
   - Blue line: Average score (last 50 episodes)
   - Red line: MA-100 (convergence metric)
   - Gold star: Best MA-100 achieved

4. **Convergence Metric**
   - Episodes without improvement counter
   - Red line: Convergence threshold (1000)
   - Orange fill: Shows "patience" usage

#### Testing Plots (2 charts):
1. **Attempt vs Max Tile Achieved**
   - Performance across multiple attempts
   - Gold star marks best achievement

2. **Attempt vs Score**
   - Blue: Score per attempt
   - Red dashed: Cumulative average

---

## 🎯 Interpreting the Plots

### Training Plot Insights

**Good Training:**
```
MA-100 steadily increasing → Agent is learning
No-Imp counter stays low → Continuous improvement
Max tiles increasing → Better gameplay strategy
```

**Plateau/Convergence:**
```
MA-100 flat or declining → Agent stopped learning
No-Imp counter approaching 1000 → Will auto-stop soon
Max tiles consistent → Skill ceiling reached
```

**Your Current Situation:**
```
Best MA-100: 1444 (around episode 1930)
Current MA-100: 1362 (episode 2620)
No-Imp counter: 694 → Will stop at 1000
Conclusion: Past peak, should use earlier checkpoint
```

### Testing Plot Insights

**Consistent Agent:**
```
Max tiles mostly similar → Reliable strategy
Scores clustered → Consistent performance
```

**Learning Needed:**
```
Max tiles highly variable → Inconsistent strategy
Low average → Agent struggling
```

---

## 🔧 Workflow

### 1. Start Training
```bash
# New terminal window
python 2048RL.py train --algorithm dqn --episodes 5000 --resume-path models/DQN/dqn_ep2000.pth
```
**Result:** Creates/appends to `evaluations/training.txt`

### 2. Monitor Training (Optional)
```bash
# In another terminal, watch the log
tail -f evaluations/training.txt

# Or check last 50 lines
tail -50 evaluations/training.txt
```

### 3. Run Challenge Mode
```bash
python 2048RL.py play --model models/DQN/dqn_ep2000.pth --challenge 2048 --no-ui
```
**Result:** Creates/appends to `evaluations/testing.txt`

### 4. Generate Plots
```bash
python plot_results.py
```
**Result:** Creates:
- `evaluations/training_plots.png`
- `evaluations/testing_plots.png`

### 5. Analyze Results
- Open the PNG files
- Check convergence status
- Identify best checkpoint
- Decide: continue training, use current model, or adjust hyperparameters

---

## 📊 Log File Locations

```
2048-Reinforcement-Learning/
├── evaluations/
│   ├── training.txt          ← All training episodes
│   ├── testing.txt           ← All testing attempts
│   ├── training_plots.png    ← Generated by plot_results.py
│   ├── testing_plots.png     ← Generated by plot_results.py
│   └── logs.txt              ← General application logs
└── plot_results.py           ← Plotting utility
```

---

## 🎮 Example Session

```bash
# Terminal 1: Start training from best checkpoint
python 2048RL.py train --algorithm dqn --episodes 3000 --resume-path models/DQN/dqn_ep2000.pth

# Terminal 2: Monitor progress
watch -n 10 "tail -20 evaluations/training.txt"

# After training stops or you stop it (Ctrl+C):
python plot_results.py --training-only

# Test the model in challenge mode
python 2048RL.py play --model models/DQN/dqn_final.pth --challenge 2048 --no-ui

# Plot testing results
python plot_results.py --testing-only

# Plot everything
python plot_results.py
```

---

## 🔍 Key Metrics Explained

| Metric | What It Means | Good Value |
|--------|---------------|------------|
| **MA-100** | Average score over last 100 episodes | Steadily increasing |
| **No-Imp** | Episodes without MA-100 improvement | Low (< 500) |
| **ε (Epsilon)** | Exploration rate (randomness) | 0.15-0.25 during training |
| **Max Tile** | Highest tile reached | 512+ is good, 1024+ is excellent |
| **Best Score** | All-time highest score | 4000+ is competitive |

---

## 💡 Tips

1. **Training logs are append-only** - Each training session adds to the file
2. **Clear old logs if needed:**
   ```bash
   rm evaluations/training.txt evaluations/testing.txt
   ```
3. **Plot anytime** - Works with partial data, updates as training progresses
4. **Compare runs** - Save plots with different names to compare settings

---

## 🚀 Ready to Use!

Everything is set up. Just run your training and the logs will automatically capture all metrics for plotting!
